//! Transaction Log Module
//!
//! This module implements persistent transaction logging for crash recovery and
//! distributed coordination. It maintains the authoritative state of all transactions
//! and enables exactly-once guarantees across broker restarts.

use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::path::PathBuf;
use std::sync::Arc;
use tokio::fs;
use tokio::sync::RwLock;
use tracing::{debug, info, warn};

use super::{
    ProducerId, TopicPartition, TransactionMetadata, TransactionResult, TxnState,
};

/// Transaction log entry types
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum TransactionLogEntry {
    /// Producer initialization or epoch bump
    InitProducerId {
        transactional_id: String,
        producer_id: ProducerId,
        producer_epoch: i16,
        transaction_timeout_ms: i32,
        timestamp: i64,
    },
    /// Transaction begun
    BeginTransaction {
        transactional_id: String,
        producer_id: ProducerId,
        producer_epoch: i16,
        timestamp: i64,
    },
    /// Partitions added to transaction
    AddPartitions {
        transactional_id: String,
        producer_id: ProducerId,
        producer_epoch: i16,
        partitions: Vec<TopicPartition>,
        timestamp: i64,
    },
    /// Consumer group offsets added to transaction
    AddOffsets {
        transactional_id: String,
        producer_id: ProducerId,
        producer_epoch: i16,
        group_id: String,
        timestamp: i64,
    },
    /// Transaction prepare phase (commit or abort)
    PrepareTransaction {
        transactional_id: String,
        producer_id: ProducerId,
        producer_epoch: i16,
        commit: bool,
        partitions: Vec<TopicPartition>,
        timestamp: i64,
    },
    /// Transaction completion (commit or abort)
    CompleteTransaction {
        transactional_id: String,
        producer_id: ProducerId,
        producer_epoch: i16,
        commit: bool,
        timestamp: i64,
    },
    /// Producer fenced
    FenceProducer {
        transactional_id: String,
        old_producer_id: ProducerId,
        old_producer_epoch: i16,
        new_producer_epoch: i16,
        timestamp: i64,
    },
}

impl TransactionLogEntry {
    /// Get timestamp of the log entry
    pub fn timestamp(&self) -> i64 {
        match self {
            Self::InitProducerId { timestamp, .. } => *timestamp,
            Self::BeginTransaction { timestamp, .. } => *timestamp,
            Self::AddPartitions { timestamp, .. } => *timestamp,
            Self::AddOffsets { timestamp, .. } => *timestamp,
            Self::PrepareTransaction { timestamp, .. } => *timestamp,
            Self::CompleteTransaction { timestamp, .. } => *timestamp,
            Self::FenceProducer { timestamp, .. } => *timestamp,
        }
    }

    /// Get transactional ID
    pub fn transactional_id(&self) -> &str {
        match self {
            Self::InitProducerId {
                transactional_id, ..
            } => transactional_id,
            Self::BeginTransaction {
                transactional_id, ..
            } => transactional_id,
            Self::AddPartitions {
                transactional_id, ..
            } => transactional_id,
            Self::AddOffsets {
                transactional_id, ..
            } => transactional_id,
            Self::PrepareTransaction {
                transactional_id, ..
            } => transactional_id,
            Self::CompleteTransaction {
                transactional_id, ..
            } => transactional_id,
            Self::FenceProducer {
                transactional_id, ..
            } => transactional_id,
        }
    }

    /// Get producer ID
    pub fn producer_id(&self) -> ProducerId {
        match self {
            Self::InitProducerId { producer_id, .. } => *producer_id,
            Self::BeginTransaction { producer_id, .. } => *producer_id,
            Self::AddPartitions { producer_id, .. } => *producer_id,
            Self::AddOffsets { producer_id, .. } => *producer_id,
            Self::PrepareTransaction { producer_id, .. } => *producer_id,
            Self::CompleteTransaction { producer_id, .. } => *producer_id,
            Self::FenceProducer {
                old_producer_id, ..
            } => *old_producer_id,
        }
    }
}

/// Transaction log for persistent transaction state
#[derive(Debug)]
pub struct TransactionLog {
    /// Log file path
    log_path: PathBuf,
    /// In-memory transaction states (for recovery and fast access)
    transaction_states: Arc<RwLock<HashMap<String, TransactionMetadata>>>,
    /// Log entry sequence number
    next_sequence_number: Arc<std::sync::atomic::AtomicU64>,
    /// Configuration
    config: TransactionLogConfig,
}

/// Transaction log configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TransactionLogConfig {
    /// Log file path
    pub log_dir: PathBuf,
    /// Maximum log file size before rotation
    pub max_log_size: u64,
    /// Number of log files to retain
    pub log_retention_count: usize,
    /// Sync to disk after every write
    pub fsync_on_write: bool,
    /// Compact log when it reaches this ratio of deleted entries
    pub compaction_threshold: f64,
}

impl Default for TransactionLogConfig {
    fn default() -> Self {
        Self {
            log_dir: PathBuf::from("./data/transaction_log"),
            max_log_size: 256 * 1024 * 1024, // 256MB
            log_retention_count: 5,
            fsync_on_write: true,
            compaction_threshold: 0.5,
        }
    }
}

impl TransactionLog {
    /// Create new transaction log
    pub async fn new(config: TransactionLogConfig) -> TransactionResult<Self> {
        // Ensure log directory exists
        fs::create_dir_all(&config.log_dir).await?;

        let log_path = config.log_dir.join("transaction.log");

        let transaction_log = Self {
            log_path,
            transaction_states: Arc::new(RwLock::new(HashMap::new())),
            next_sequence_number: Arc::new(std::sync::atomic::AtomicU64::new(0)),
            config,
        };

        // Load existing log entries
        transaction_log.recover().await?;

        info!(
            log_path = ?transaction_log.log_path,
            "Transaction log initialized"
        );

        Ok(transaction_log)
    }

    /// Log producer initialization
    pub async fn log_init_producer_id(
        &self,
        transactional_id: String,
        producer_id: ProducerId,
        producer_epoch: i16,
        transaction_timeout_ms: i32,
    ) -> TransactionResult<()> {
        let entry = TransactionLogEntry::InitProducerId {
            transactional_id: transactional_id.clone(),
            producer_id,
            producer_epoch,
            transaction_timeout_ms,
            timestamp: chrono::Utc::now().timestamp_millis(),
        };

        self.append_log_entry(entry).await?;

        // Update in-memory state
        let mut states = self.transaction_states.write().await;
        let metadata = TransactionMetadata {
            transactional_id: transactional_id.clone(),
            producer_id,
            producer_epoch,
            state: TxnState::Empty,
            partitions: std::collections::HashSet::new(),
            transaction_timeout_ms,
            txn_start_timestamp: chrono::Utc::now().timestamp_millis(),
            txn_last_update_timestamp: chrono::Utc::now().timestamp_millis(),
        };
        states.insert(transactional_id, metadata);

        debug!(
            producer_id = producer_id,
            producer_epoch = producer_epoch,
            "Logged producer initialization"
        );

        Ok(())
    }

    /// Log transaction begin
    pub async fn log_begin_transaction(
        &self,
        transactional_id: &str,
        producer_id: ProducerId,
        producer_epoch: i16,
    ) -> TransactionResult<()> {
        let entry = TransactionLogEntry::BeginTransaction {
            transactional_id: transactional_id.to_string(),
            producer_id,
            producer_epoch,
            timestamp: chrono::Utc::now().timestamp_millis(),
        };

        self.append_log_entry(entry).await?;

        // Update in-memory state
        let mut states = self.transaction_states.write().await;
        if let Some(metadata) = states.get_mut(transactional_id) {
            metadata.state = TxnState::Ongoing;
            metadata.txn_start_timestamp = chrono::Utc::now().timestamp_millis();
            metadata.txn_last_update_timestamp = chrono::Utc::now().timestamp_millis();
            metadata.partitions.clear();
        }

        debug!(
            transactional_id = transactional_id,
            producer_id = producer_id,
            "Logged transaction begin"
        );

        Ok(())
    }

    /// Log partitions added to transaction
    pub async fn log_add_partitions(
        &self,
        transactional_id: &str,
        producer_id: ProducerId,
        producer_epoch: i16,
        partitions: Vec<TopicPartition>,
    ) -> TransactionResult<()> {
        let entry = TransactionLogEntry::AddPartitions {
            transactional_id: transactional_id.to_string(),
            producer_id,
            producer_epoch,
            partitions: partitions.clone(),
            timestamp: chrono::Utc::now().timestamp_millis(),
        };

        self.append_log_entry(entry).await?;

        let partition_count = partitions.len();

        // Update in-memory state
        let mut states = self.transaction_states.write().await;
        if let Some(metadata) = states.get_mut(transactional_id) {
            for partition in partitions {
                metadata.partitions.insert(partition);
            }
            metadata.txn_last_update_timestamp = chrono::Utc::now().timestamp_millis();
        }

        debug!(
            transactional_id = transactional_id,
            producer_id = producer_id,
            partition_count = partition_count,
            "Logged add partitions"
        );

        Ok(())
    }

    /// Log prepare transaction (commit or abort)
    pub async fn log_prepare_transaction(
        &self,
        transactional_id: &str,
        producer_id: ProducerId,
        producer_epoch: i16,
        commit: bool,
        partitions: Vec<TopicPartition>,
    ) -> TransactionResult<()> {
        let entry = TransactionLogEntry::PrepareTransaction {
            transactional_id: transactional_id.to_string(),
            producer_id,
            producer_epoch,
            commit,
            partitions,
            timestamp: chrono::Utc::now().timestamp_millis(),
        };

        self.append_log_entry(entry).await?;

        // Update in-memory state
        let mut states = self.transaction_states.write().await;
        if let Some(metadata) = states.get_mut(transactional_id) {
            metadata.state = if commit {
                TxnState::PrepareCommit
            } else {
                TxnState::PrepareAbort
            };
            metadata.txn_last_update_timestamp = chrono::Utc::now().timestamp_millis();
        }

        debug!(
            transactional_id = transactional_id,
            producer_id = producer_id,
            commit = commit,
            "Logged prepare transaction"
        );

        Ok(())
    }

    /// Log complete transaction (commit or abort)
    pub async fn log_complete_transaction(
        &self,
        transactional_id: &str,
        producer_id: ProducerId,
        producer_epoch: i16,
        commit: bool,
    ) -> TransactionResult<()> {
        let entry = TransactionLogEntry::CompleteTransaction {
            transactional_id: transactional_id.to_string(),
            producer_id,
            producer_epoch,
            commit,
            timestamp: chrono::Utc::now().timestamp_millis(),
        };

        self.append_log_entry(entry).await?;

        // Update in-memory state
        let mut states = self.transaction_states.write().await;
        if let Some(metadata) = states.get_mut(transactional_id) {
            metadata.state = if commit {
                TxnState::CompleteCommit
            } else {
                TxnState::CompleteAbort
            };
            metadata.txn_last_update_timestamp = chrono::Utc::now().timestamp_millis();
        }

        info!(
            transactional_id = transactional_id,
            producer_id = producer_id,
            commit = commit,
            "Logged complete transaction"
        );

        Ok(())
    }

    /// Get transaction metadata by transactional ID
    pub async fn get_transaction_metadata(
        &self,
        transactional_id: &str,
    ) -> Option<TransactionMetadata> {
        let states = self.transaction_states.read().await;
        states.get(transactional_id).cloned()
    }

    /// Get all active transactions
    pub async fn get_active_transactions(&self) -> HashMap<String, TransactionMetadata> {
        let states = self.transaction_states.read().await;
        states
            .iter()
            .filter(|(_, metadata)| !metadata.state.is_terminal())
            .map(|(k, v)| (k.clone(), v.clone()))
            .collect()
    }

    /// Remove completed transactions from memory
    pub async fn cleanup_completed_transactions(&self) -> usize {
        let mut states = self.transaction_states.write().await;
        let initial_count = states.len();

        states.retain(|_, metadata| !metadata.state.is_terminal());

        let cleaned_up = initial_count - states.len();
        if cleaned_up > 0 {
            info!(cleaned_up = cleaned_up, "Cleaned up completed transactions");
        }

        cleaned_up
    }

    /// Append log entry to persistent log
    async fn append_log_entry(&self, entry: TransactionLogEntry) -> TransactionResult<()> {
        let sequence_number = self
            .next_sequence_number
            .fetch_add(1, std::sync::atomic::Ordering::SeqCst);

        let log_record = LogRecord {
            sequence_number,
            entry,
        };

        let serialized = serde_json::to_string(&log_record)?;
        let log_line = format!("{}\n", serialized);

        let mut file = tokio::fs::OpenOptions::new()
            .create(true)
            .append(true)
            .open(&self.log_path)
            .await?;

        use tokio::io::AsyncWriteExt;
        file.write_all(log_line.as_bytes()).await?;

        if self.config.fsync_on_write {
            file.sync_all().await?;
        }

        debug!(
            sequence_number = sequence_number,
            "Appended transaction log entry"
        );

        Ok(())
    }

    /// Recover transaction state from log
    async fn recover(&self) -> TransactionResult<()> {
        if !self.log_path.exists() {
            info!("No existing transaction log found, starting fresh");
            return Ok(());
        }

        let content = fs::read_to_string(&self.log_path).await?;
        let mut recovered_transactions = HashMap::new();
        let mut max_sequence = 0u64;

        for (line_num, line) in content.lines().enumerate() {
            if line.trim().is_empty() {
                continue;
            }

            match serde_json::from_str::<LogRecord>(line) {
                Ok(record) => {
                    max_sequence = max_sequence.max(record.sequence_number);
                    self.apply_log_entry(record.entry, &mut recovered_transactions);
                }
                Err(e) => {
                    warn!(
                        line_number = line_num + 1,
                        error = %e,
                        "Failed to parse log entry, skipping"
                    );
                }
            }
        }

        // Update in-memory state
        let mut states = self.transaction_states.write().await;
        *states = recovered_transactions;

        self.next_sequence_number
            .store(max_sequence + 1, std::sync::atomic::Ordering::SeqCst);

        info!(
            recovered_transactions = states.len(),
            max_sequence = max_sequence,
            "Transaction log recovery completed"
        );

        Ok(())
    }

    /// Apply log entry during recovery
    fn apply_log_entry(
        &self,
        entry: TransactionLogEntry,
        states: &mut HashMap<String, TransactionMetadata>,
    ) {
        match entry {
            TransactionLogEntry::InitProducerId {
                transactional_id,
                producer_id,
                producer_epoch,
                transaction_timeout_ms,
                timestamp,
            } => {
                let metadata = TransactionMetadata {
                    transactional_id: transactional_id.clone(),
                    producer_id,
                    producer_epoch,
                    state: TxnState::Empty,
                    partitions: std::collections::HashSet::new(),
                    transaction_timeout_ms,
                    txn_start_timestamp: timestamp,
                    txn_last_update_timestamp: timestamp,
                };
                states.insert(transactional_id, metadata);
            }
            TransactionLogEntry::BeginTransaction {
                transactional_id,
                timestamp,
                ..
            } => {
                if let Some(metadata) = states.get_mut(&transactional_id) {
                    metadata.state = TxnState::Ongoing;
                    metadata.txn_start_timestamp = timestamp;
                    metadata.txn_last_update_timestamp = timestamp;
                    metadata.partitions.clear();
                }
            }
            TransactionLogEntry::AddPartitions {
                transactional_id,
                partitions,
                timestamp,
                ..
            } => {
                if let Some(metadata) = states.get_mut(&transactional_id) {
                    for partition in partitions {
                        metadata.partitions.insert(partition);
                    }
                    metadata.txn_last_update_timestamp = timestamp;
                }
            }
            TransactionLogEntry::PrepareTransaction {
                transactional_id,
                commit,
                timestamp,
                ..
            } => {
                if let Some(metadata) = states.get_mut(&transactional_id) {
                    metadata.state = if commit {
                        TxnState::PrepareCommit
                    } else {
                        TxnState::PrepareAbort
                    };
                    metadata.txn_last_update_timestamp = timestamp;
                }
            }
            TransactionLogEntry::CompleteTransaction {
                transactional_id,
                commit,
                timestamp,
                ..
            } => {
                if let Some(metadata) = states.get_mut(&transactional_id) {
                    metadata.state = if commit {
                        TxnState::CompleteCommit
                    } else {
                        TxnState::CompleteAbort
                    };
                    metadata.txn_last_update_timestamp = timestamp;
                }
            }
            TransactionLogEntry::FenceProducer {
                transactional_id,
                new_producer_epoch,
                timestamp,
                ..
            } => {
                if let Some(metadata) = states.get_mut(&transactional_id) {
                    metadata.producer_epoch = new_producer_epoch;
                    metadata.state = TxnState::Dead;
                    metadata.txn_last_update_timestamp = timestamp;
                }
            }
            _ => {
                // Handle other entry types as needed
            }
        }
    }
}

/// Log record with sequence number
#[derive(Debug, Serialize, Deserialize)]
struct LogRecord {
    sequence_number: u64,
    entry: TransactionLogEntry,
}

#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::TempDir;

    async fn create_test_log() -> (TransactionLog, TempDir) {
        let temp_dir = TempDir::new().unwrap();
        let config = TransactionLogConfig {
            log_dir: temp_dir.path().to_path_buf(),
            ..Default::default()
        };
        let log = TransactionLog::new(config).await.unwrap();
        (log, temp_dir)
    }

    #[tokio::test]
    async fn test_log_init_producer_id() {
        let (log, _temp_dir) = create_test_log().await;

        log.log_init_producer_id("test-txn".to_string(), 1, 0, 60000)
            .await
            .unwrap();

        let metadata = log.get_transaction_metadata("test-txn").await.unwrap();
        assert_eq!(metadata.producer_id, 1);
        assert_eq!(metadata.producer_epoch, 0);
        assert_eq!(metadata.state, TxnState::Empty);
    }

    #[tokio::test]
    async fn test_transaction_lifecycle_logging() {
        let (log, _temp_dir) = create_test_log().await;

        // Initialize producer
        log.log_init_producer_id("test-txn".to_string(), 1, 0, 60000)
            .await
            .unwrap();

        // Begin transaction
        log.log_begin_transaction("test-txn", 1, 0).await.unwrap();
        let metadata = log.get_transaction_metadata("test-txn").await.unwrap();
        assert_eq!(metadata.state, TxnState::Ongoing);

        // Add partitions
        let partitions = vec![TopicPartition::new("topic1".to_string(), 0)];
        log.log_add_partitions("test-txn", 1, 0, partitions)
            .await
            .unwrap();
        let metadata = log.get_transaction_metadata("test-txn").await.unwrap();
        assert_eq!(metadata.partitions.len(), 1);

        // Prepare commit
        log.log_prepare_transaction("test-txn", 1, 0, true, vec![])
            .await
            .unwrap();
        let metadata = log.get_transaction_metadata("test-txn").await.unwrap();
        assert_eq!(metadata.state, TxnState::PrepareCommit);

        // Complete commit
        log.log_complete_transaction("test-txn", 1, 0, true)
            .await
            .unwrap();
        let metadata = log.get_transaction_metadata("test-txn").await.unwrap();
        assert_eq!(metadata.state, TxnState::CompleteCommit);
    }

    #[tokio::test]
    async fn test_log_recovery() {
        let temp_dir = TempDir::new().unwrap();
        let config = TransactionLogConfig {
            log_dir: temp_dir.path().to_path_buf(),
            ..Default::default()
        };

        // Create log and add some entries
        {
            let log = TransactionLog::new(config.clone()).await.unwrap();
            log.log_init_producer_id("test-txn".to_string(), 1, 0, 60000)
                .await
                .unwrap();
            log.log_begin_transaction("test-txn", 1, 0).await.unwrap();
        }

        // Create new log instance to test recovery
        let log = TransactionLog::new(config).await.unwrap();
        let metadata = log.get_transaction_metadata("test-txn").await.unwrap();
        assert_eq!(metadata.producer_id, 1);
        assert_eq!(metadata.state, TxnState::Ongoing);
    }

    #[tokio::test]
    async fn test_cleanup_completed_transactions() {
        let (log, _temp_dir) = create_test_log().await;

        // Add completed transaction
        log.log_init_producer_id("txn1".to_string(), 1, 0, 60000)
            .await
            .unwrap();
        log.log_complete_transaction("txn1", 1, 0, true)
            .await
            .unwrap();

        // Add ongoing transaction
        log.log_init_producer_id("txn2".to_string(), 2, 0, 60000)
            .await
            .unwrap();
        log.log_begin_transaction("txn2", 2, 0).await.unwrap();

        assert_eq!(log.cleanup_completed_transactions().await, 1);

        // Only ongoing transaction should remain
        assert!(log.get_transaction_metadata("txn1").await.is_none());
        assert!(log.get_transaction_metadata("txn2").await.is_some());
    }
}
